{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa33eb2",
   "metadata": {},
   "source": [
    "# HPO Job Runner for HPC\n",
    "\n",
    "**Purpose**: Submit and manage hyperparameter optimization jobs on SLURM cluster\n",
    "\n",
    "**Date**: January 16, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook helps you:\n",
    "1. Submit individual HPO jobs to SLURM\n",
    "2. Submit batch experiments (all models for a dataset)\n",
    "3. Monitor running jobs\n",
    "4. Check job status and logs\n",
    "5. Analyze completed results\n",
    "\n",
    "**Models**: NHITS_Q, TFT_Q, TIMESNET_Q  \n",
    "**Datasets**: heat, water_centrum, water_tommerby  \n",
    "**Total Experiments**: 9 (3 models √ó 3 datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707dadac",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "272a0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Working directory: /home/hpc/iwi5/iwi5389h/ExAI-Timeseries-Thesis\n",
      "‚úÖ Models: NHITS_Q, TFT_Q, TIMESNET_Q\n",
      "‚úÖ Datasets: heat, water_centrum, water_tommerby\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/hpc/iwi5/iwi5389h/ExAI-Timeseries-Thesis')\n",
    "print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Configuration\n",
    "MODELS = ['NHITS_Q', 'TFT_Q', 'TIMESNET_Q']\n",
    "DATASETS = ['heat', 'water_centrum', 'water_tommerby']\n",
    "DEFAULT_TRIALS = 50\n",
    "\n",
    "print(f\"‚úÖ Models: {', '.join(MODELS)}\")\n",
    "print(f\"‚úÖ Datasets: {', '.join(DATASETS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9450fe5",
   "metadata": {},
   "source": [
    "## üöÄ Job Submission Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d13978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Job submission functions loaded\n"
     ]
    }
   ],
   "source": [
    "def submit_single_job(model, dataset, trials=50, dry_run=False):\n",
    "    \"\"\"\n",
    "    Submit a single HPO job to SLURM\n",
    "    \n",
    "    Args:\n",
    "        model: Model name (NHITS_Q, TFT_Q, TIMESNET_Q)\n",
    "        dataset: Dataset name (heat, water_centrum, water_tommerby)\n",
    "        trials: Number of optimization trials (default: 50)\n",
    "        dry_run: If True, print command without executing\n",
    "    \n",
    "    Returns:\n",
    "        Job ID if submitted, None otherwise\n",
    "    \"\"\"\n",
    "    cmd = f\"./hpo/submit_job.sh {model} {dataset} {trials}\"\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"[DRY RUN] Would execute: {cmd}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        # Parse job ID from output: \"Submitted batch job 1234567\"\n",
    "        output = result.stdout.strip()\n",
    "        job_id = None\n",
    "        \n",
    "        for line in output.split('\\n'):\n",
    "            if \"Submitted batch job\" in line:\n",
    "                # Extract just the job ID number\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    job_id = parts[3]\n",
    "                    break\n",
    "        \n",
    "        if job_id:\n",
    "            print(f\"‚úÖ Submitted {model} on {dataset} - Job ID: {job_id}\")\n",
    "            return job_id\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Job submitted but couldn't extract ID\")\n",
    "            print(f\"   Full output:\\n{output}\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to submit {model} on {dataset}\")\n",
    "        print(f\"   stdout: {e.stdout}\")\n",
    "        print(f\"   stderr: {e.stderr}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def submit_batch_jobs(models, datasets, trials=50, delay=5, dry_run=False):\n",
    "    \"\"\"\n",
    "    Submit multiple HPO jobs with delay between submissions\n",
    "    \n",
    "    Args:\n",
    "        models: List of model names\n",
    "        datasets: List of dataset names\n",
    "        trials: Number of trials per job\n",
    "        delay: Seconds to wait between submissions\n",
    "        dry_run: If True, print commands without executing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping (model, dataset) to job_id\n",
    "    \"\"\"\n",
    "    job_ids = {}\n",
    "    total = len(models) * len(datasets)\n",
    "    current = 0\n",
    "    \n",
    "    print(f\"üìä Submitting {total} jobs...\\n\")\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            current += 1\n",
    "            print(f\"[{current}/{total}] {model} on {dataset}\")\n",
    "            \n",
    "            job_id = submit_single_job(model, dataset, trials, dry_run)\n",
    "            if job_id:\n",
    "                job_ids[(model, dataset)] = job_id\n",
    "            \n",
    "            # Wait between submissions to avoid overwhelming scheduler\n",
    "            if current < total and not dry_run:\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        print()  # Blank line between datasets\n",
    "    \n",
    "    print(f\"\\n‚úÖ Submitted {len(job_ids)}/{total} jobs successfully\")\n",
    "    return job_ids\n",
    "\n",
    "\n",
    "def save_job_tracker(job_ids, filename='hpo/hpo_current_jobs.json'):\n",
    "    \"\"\"\n",
    "    Save submitted job IDs for later tracking\n",
    "    \n",
    "    Args:\n",
    "        job_ids: Dictionary mapping (model, dataset) to job_id\n",
    "        filename: Path to save JSON file\n",
    "    \"\"\"\n",
    "    # Convert tuple keys to string for JSON serialization\n",
    "    serializable = {\n",
    "        f\"{model}_{dataset}\": {\n",
    "            'job_id': job_id,\n",
    "            'model': model,\n",
    "            'dataset': dataset,\n",
    "            'submitted_at': datetime.now().isoformat()\n",
    "        }\n",
    "        for (model, dataset), job_id in job_ids.items()\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(serializable, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Saved job tracker to {filename}\")\n",
    "\n",
    "print(\"‚úÖ Job submission functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9feb",
   "metadata": {},
   "source": [
    "## üìä Job Monitoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f53db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Job monitoring functions loaded\n"
     ]
    }
   ],
   "source": [
    "def get_job_status(job_id=None):\n",
    "    \"\"\"\n",
    "    Get status of SLURM jobs\n",
    "    \n",
    "    Args:\n",
    "        job_id: Specific job ID to check, or None for all user jobs\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with job information\n",
    "    \"\"\"\n",
    "    if job_id:\n",
    "        cmd = f\"squeue -j {job_id} -o '%.18i %.9P %.30j %.8u %.2t %.10M %.6D %R'\"\n",
    "    else:\n",
    "        cmd = \"squeue -u $USER -o '%.18i %.9P %.30j %.8u %.2t %.10M %.6D %R'\"\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        if len(lines) <= 1:\n",
    "            print(\"No jobs found\")\n",
    "            return None\n",
    "        \n",
    "        # Parse output into DataFrame\n",
    "        header = lines[0].split()\n",
    "        data = [line.split(None, len(header)-1) for line in lines[1:]]\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=header)\n",
    "        return df\n",
    "        \n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"‚ùå Failed to get job status\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_hpo_jobs():\n",
    "    \"\"\"\n",
    "    Check status of all HPO jobs (jobs with 'hpo_' prefix)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with HPO job information\n",
    "    \"\"\"\n",
    "    df = get_job_status()\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Filter for HPO jobs\n",
    "    hpo_jobs = df[df['NAME'].str.contains('hpo_', na=False)]\n",
    "    \n",
    "    if len(hpo_jobs) == 0:\n",
    "        print(\"No HPO jobs currently running\")\n",
    "        return None\n",
    "    \n",
    "    return hpo_jobs\n",
    "\n",
    "\n",
    "def tail_log(model, dataset, job_id, lines=20, log_type='log'):\n",
    "    \"\"\"\n",
    "    Display last N lines of a job's log file\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        dataset: Dataset name\n",
    "        job_id: Job ID\n",
    "        lines: Number of lines to show\n",
    "        log_type: 'log' for stdout or 'err' for stderr\n",
    "    \"\"\"\n",
    "    extension = 'log' if log_type == 'log' else 'err'\n",
    "    log_file = f\"hpo/logs/hpo_{model}_{dataset}_{job_id}.{extension}\"\n",
    "    \n",
    "    if not os.path.exists(log_file):\n",
    "        print(f\"‚ùå Log file not found: {log_file}\")\n",
    "        # Try to find similar files\n",
    "        pattern = f\"hpo/logs/hpo_{model}_{dataset}_*.{extension}\"\n",
    "        matches = glob(pattern)\n",
    "        if matches:\n",
    "            print(f\"\\nüí° Found similar files:\")\n",
    "            for match in matches:\n",
    "                print(f\"   {match}\")\n",
    "        return\n",
    "    \n",
    "    cmd = f\"tail -n {lines} {log_file}\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    log_type_name = \"stdout\" if log_type == 'log' else \"stderr\"\n",
    "    print(f\"üìÑ Last {lines} lines of {log_file} ({log_type_name}):\\n\")\n",
    "    print(result.stdout)\n",
    "\n",
    "\n",
    "def view_logs(model, dataset, job_id, lines=30):\n",
    "    \"\"\"\n",
    "    Display both stdout and stderr logs for a job\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        dataset: Dataset name\n",
    "        job_id: Job ID\n",
    "        lines: Number of lines to show from each log\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"LOGS FOR: {model} on {dataset} (Job {job_id})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check stdout log\n",
    "    print(\"\\nüìä STDOUT LOG:\")\n",
    "    print(\"-\"*80)\n",
    "    tail_log(model, dataset, job_id, lines, 'log')\n",
    "    \n",
    "    # Check stderr log\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ö†Ô∏è  STDERR LOG:\")\n",
    "    print(\"-\"*80)\n",
    "    tail_log(model, dataset, job_id, lines, 'err')\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "def cancel_job(job_id):\n",
    "    \"\"\"\n",
    "    Cancel a SLURM job\n",
    "    \n",
    "    Args:\n",
    "        job_id: Job ID to cancel\n",
    "    \"\"\"\n",
    "    cmd = f\"scancel {job_id}\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Cancelled job {job_id}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to cancel job {job_id}\")\n",
    "        print(result.stderr)\n",
    "\n",
    "print(\"‚úÖ Job monitoring functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699c644",
   "metadata": {},
   "source": [
    "## üéØ Quick Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49f876",
   "metadata": {},
   "source": [
    "### Option 1: Submit Single Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c91e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Submitted TIMESNET_Q on water_tommerby - Job ID: 1511651\n"
     ]
    }
   ],
   "source": [
    "# Submit a single job\n",
    "# Modify these values as needed:\n",
    "\n",
    "MODEL = 'TIMESNET_Q'  # NHITS_Q, TFT_Q, or TIMESNET_Q\n",
    "DATASET = 'water_tommerby'   # heat, water_centrum, or water_tommerby\n",
    "TRIALS = 50        # Number of optimization trials\n",
    "DRY_RUN = False     # Set to False to actually submit\n",
    "\n",
    "job_id = submit_single_job(MODEL, DATASET, TRIALS, dry_run=DRY_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98646a42",
   "metadata": {},
   "source": [
    "### Option 2: Submit All Jobs for One Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06205e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit all 3 models for a specific dataset\n",
    "\n",
    "DATASET = 'heat'   # heat, water_centrum, or water_tommerby\n",
    "TRIALS = 50\n",
    "DRY_RUN = True     # Set to False to actually submit\n",
    "\n",
    "job_ids = submit_batch_jobs(\n",
    "    models=MODELS,\n",
    "    datasets=[DATASET],\n",
    "    trials=TRIALS,\n",
    "    delay=5,\n",
    "    dry_run=DRY_RUN\n",
    ")\n",
    "\n",
    "if not DRY_RUN and job_ids:\n",
    "    save_job_tracker(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eafa2a",
   "metadata": {},
   "source": [
    "### Option 3: Submit ALL 9 Experiments (Priority Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f231adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Submit all experiments in priority order:\n",
    "# # Priority 1: heat (most critical)\n",
    "# # Priority 2: water_centrum\n",
    "# # Priority 3: water_tommerby\n",
    "\n",
    "# TRIALS = 50\n",
    "# DRY_RUN = True  # Set to False to actually submit\n",
    "\n",
    "# PRIORITY_ORDER = ['heat', 'water_centrum', 'water_tommerby']\n",
    "\n",
    "# all_job_ids = submit_batch_jobs(\n",
    "#     models=MODELS,\n",
    "#     datasets=PRIORITY_ORDER,\n",
    "#     trials=TRIALS,\n",
    "#     delay=5,\n",
    "#     dry_run=DRY_RUN\n",
    "# )\n",
    "\n",
    "# if not DRY_RUN and all_job_ids:\n",
    "#     save_job_tracker(all_job_ids)\n",
    "#     print(f\"\\nüìä Total jobs submitted: {len(all_job_ids)}\")\n",
    "#     print(f\"‚è±Ô∏è  Estimated total GPU hours: 80-100 hours\")\n",
    "#     print(f\"üíæ Job tracker saved to hpo/hpo_current_jobs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c84a5",
   "metadata": {},
   "source": [
    "## üìà Monitor Running Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7920290",
   "metadata": {},
   "source": [
    "### Check All User Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3007223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Total active jobs: 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOBID</th>\n",
       "      <th>PARTITION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>USER</th>\n",
       "      <th>ST</th>\n",
       "      <th>TIME</th>\n",
       "      <th>NODES</th>\n",
       "      <th>NODELIST(REASON)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1511651</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_TIMESNET_Q_water_tommerby</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1511650</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_TFT_Q_water_tommerby</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1511649</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_NHITS_Q_water_tommerby</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1511648</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_TIMESNET_Q_water_centrum</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1511647</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_TFT_Q_water_centrum</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1511646</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_NHITS_Q_water_centrum</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1511645</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_TIMESNET_Q_heat</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1511644</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_TFT_Q_heat</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1511642</td>\n",
       "      <td>a100</td>\n",
       "      <td>hpo_NHITS_Q_heat</td>\n",
       "      <td>iwi5389h</td>\n",
       "      <td>PD</td>\n",
       "      <td>0:00</td>\n",
       "      <td>1</td>\n",
       "      <td>(Priority)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     JOBID PARTITION                           NAME      USER  ST  TIME NODES  \\\n",
       "0  1511651      a100  hpo_TIMESNET_Q_water_tommerby  iwi5389h  PD  0:00     1   \n",
       "1  1511650      a100       hpo_TFT_Q_water_tommerby  iwi5389h  PD  0:00     1   \n",
       "2  1511649      a100     hpo_NHITS_Q_water_tommerby  iwi5389h  PD  0:00     1   \n",
       "3  1511648      a100   hpo_TIMESNET_Q_water_centrum  iwi5389h  PD  0:00     1   \n",
       "4  1511647      a100        hpo_TFT_Q_water_centrum  iwi5389h  PD  0:00     1   \n",
       "5  1511646      a100      hpo_NHITS_Q_water_centrum  iwi5389h  PD  0:00     1   \n",
       "6  1511645      a100            hpo_TIMESNET_Q_heat  iwi5389h  PD  0:00     1   \n",
       "7  1511644      a100                 hpo_TFT_Q_heat  iwi5389h  PD  0:00     1   \n",
       "8  1511642      a100               hpo_NHITS_Q_heat  iwi5389h  PD  0:00     1   \n",
       "\n",
       "  NODELIST(REASON)  \n",
       "0       (Priority)  \n",
       "1       (Priority)  \n",
       "2       (Priority)  \n",
       "3       (Priority)  \n",
       "4       (Priority)  \n",
       "5       (Priority)  \n",
       "6       (Priority)  \n",
       "7       (Priority)  \n",
       "8       (Priority)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check all your running jobs\n",
    "df = get_job_status()\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nüìä Total active jobs: {len(df)}\\n\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55253516",
   "metadata": {},
   "source": [
    "### Check HPO Jobs Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "773bd4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No jobs found\n"
     ]
    }
   ],
   "source": [
    "# Check only HPO-related jobs\n",
    "hpo_df = check_hpo_jobs()\n",
    "\n",
    "if hpo_df is not None:\n",
    "    print(f\"\\nüî¨ HPO jobs running: {len(hpo_df)}\\n\")\n",
    "    display(hpo_df)\n",
    "    \n",
    "    # Count by status\n",
    "    status_counts = hpo_df['ST'].value_counts()\n",
    "    print(\"\\nStatus breakdown:\")\n",
    "    for status, count in status_counts.items():\n",
    "        status_name = {'R': 'Running', 'PD': 'Pending', 'CG': 'Completing'}.get(status, status)\n",
    "        print(f\"  {status_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0cec3",
   "metadata": {},
   "source": [
    "### View Job Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View last 30 lines of a job's logs (both stdout and stderr)\n",
    "# Modify these values:\n",
    "\n",
    "MODEL = 'NHITS_Q'\n",
    "DATASET = 'heat'\n",
    "JOB_ID = '1234567'  # Replace with actual job ID\n",
    "LINES = 30\n",
    "\n",
    "# Option 1: View both stdout and stderr\n",
    "view_logs(MODEL, DATASET, JOB_ID, LINES)\n",
    "\n",
    "# Option 2: View only stdout\n",
    "# tail_log(MODEL, DATASET, JOB_ID, LINES, 'log')\n",
    "\n",
    "# Option 3: View only stderr\n",
    "# tail_log(MODEL, DATASET, JOB_ID, LINES, 'err')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494b5ab",
   "metadata": {},
   "source": [
    "### Load Tracked Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e37ce686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No tracked jobs file found\n",
      "   Submit jobs first to create tracking file\n"
     ]
    }
   ],
   "source": [
    "# Load previously submitted jobs from tracker file\n",
    "tracker_file = 'hpo/hpo_current_jobs.json'\n",
    "\n",
    "if os.path.exists(tracker_file):\n",
    "    with open(tracker_file) as f:\n",
    "        tracked_jobs = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Tracked jobs: {len(tracked_jobs)}\\n\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for key, info in tracked_jobs.items():\n",
    "        summary_data.append({\n",
    "            'Model': info['model'],\n",
    "            'Dataset': info['dataset'],\n",
    "            'Job ID': info['job_id'],\n",
    "            'Submitted': info['submitted_at'][:19]  # Remove milliseconds\n",
    "        })\n",
    "    \n",
    "    df_tracked = pd.DataFrame(summary_data)\n",
    "    display(df_tracked)\n",
    "else:\n",
    "    print(\"‚ùå No tracked jobs file found\")\n",
    "    print(\"   Submit jobs first to create tracking file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee87eb6",
   "metadata": {},
   "source": [
    "## üóëÔ∏è Job Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e3402",
   "metadata": {},
   "source": [
    "### Cancel a Specific Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel a job by ID\n",
    "JOB_ID = '1234567'  # Replace with actual job ID\n",
    "\n",
    "# Uncomment to execute:\n",
    "# cancel_job(JOB_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc36dd0",
   "metadata": {},
   "source": [
    "### Cancel All HPO Jobs (DANGEROUS!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0d37f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Set CONFIRM=True to cancel all HPO jobs\n"
     ]
    }
   ],
   "source": [
    "# Cancel ALL HPO jobs\n",
    "# WARNING: This will cancel all running HPO experiments!\n",
    "\n",
    "CONFIRM = False  # Set to True to execute\n",
    "\n",
    "if CONFIRM:\n",
    "    hpo_df = check_hpo_jobs()\n",
    "    if hpo_df is not None:\n",
    "        job_ids = hpo_df['JOBID'].tolist()\n",
    "        print(f\"‚ö†Ô∏è  Cancelling {len(job_ids)} HPO jobs...\\n\")\n",
    "        \n",
    "        for job_id in job_ids:\n",
    "            cancel_job(job_id)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Cancelled {len(job_ids)} jobs\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Set CONFIRM=True to cancel all HPO jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ccc6a",
   "metadata": {},
   "source": [
    "## üìä Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee50b7c",
   "metadata": {},
   "source": [
    "### Check Completed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all completed HPO results\n",
    "result_files = glob('hpo/results/*/best_params_*.json')\n",
    "\n",
    "print(f\"üìä Found {len(result_files)} completed HPO results\\n\")\n",
    "\n",
    "if result_files:\n",
    "    results_summary = []\n",
    "    \n",
    "    for result_file in sorted(result_files):\n",
    "        with open(result_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract info from filename: best_params_MODEL_DATASET_JOBID.json\n",
    "        filename = os.path.basename(result_file)\n",
    "        parts = filename.replace('best_params_', '').replace('.json', '').split('_')\n",
    "        \n",
    "        # Handle model names with underscores (e.g., NHITS_Q)\n",
    "        if len(parts) >= 3:\n",
    "            # Reconstruct model name (everything except last 2 parts)\n",
    "            dataset = parts[-2]\n",
    "            job_id = parts[-1]\n",
    "            model = '_'.join(parts[:-2])\n",
    "        else:\n",
    "            model = dataset = job_id = 'unknown'\n",
    "        \n",
    "        balanced = data.get('best_balanced', {})\n",
    "        best_mae = data.get('best_mae', {})\n",
    "        \n",
    "        results_summary.append({\n",
    "            'Model': model,\n",
    "            'Dataset': dataset,\n",
    "            'Job ID': job_id,\n",
    "            'Best MAE': f\"{best_mae.get('mae', 999):.3f}\",\n",
    "            'PICP (%)': f\"{best_mae.get('picp', 0)*100:.1f}\",\n",
    "            'Balanced MAE': f\"{balanced.get('mae', 999):.3f}\",\n",
    "            'Balanced PICP (%)': f\"{balanced.get('picp', 0)*100:.1f}\",\n",
    "            'Pareto Solutions': data.get('num_pareto_solutions', 0)\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_summary)\n",
    "    display(df_results)\n",
    "    \n",
    "    print(f\"\\nüíæ Result files located in: hpo/results/*/\")\n",
    "else:\n",
    "    print(\"‚ùå No completed results found yet\")\n",
    "    print(\"   Results will appear in hpo/results/ after jobs complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1e569",
   "metadata": {},
   "source": [
    "### Run Full Analysis Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comprehensive analysis script\n",
    "!python hpo/analyze_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffea61",
   "metadata": {},
   "source": [
    "## üìù Notes\n",
    "\n",
    "### Estimated Runtime per Job\n",
    "- **NHITS_Q**: ~20-25 minutes per trial ‚Üí **16-20 GPU hours** for 50 trials\n",
    "- **TFT_Q**: ~30-35 minutes per trial ‚Üí **25-30 GPU hours** for 50 trials\n",
    "- **TIMESNET_Q**: ~20-25 minutes per trial ‚Üí **16-20 GPU hours** for 50 trials\n",
    "\n",
    "### Priority Recommendations\n",
    "1. **Heat dataset first** (primary thesis dataset)\n",
    "2. **Water centrum second** (good validation dataset)\n",
    "3. **Water tommerby last** (additional validation)\n",
    "\n",
    "### Troubleshooting\n",
    "- If jobs fail immediately, check logs: `tail -f hpo/logs/hpo_MODEL_DATASET_JOBID.log`\n",
    "- If jobs are pending forever, check cluster status: `sinfo -p gpu`\n",
    "- For out of memory errors, reduce batch_size in search space\n",
    "\n",
    "### Resource Allocation\n",
    "- **GPU**: 1√ó NVIDIA A100 per job\n",
    "- **RAM**: 64GB per job\n",
    "- **Time limit**: 12 hours per job\n",
    "- **Partition**: gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
