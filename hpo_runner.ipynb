{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20b986f",
   "metadata": {},
   "source": [
    "## 1. Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d437fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4475d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b0498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set: ['NHITS', 'TIMESNET'] 50\n"
     ]
    }
   ],
   "source": [
    "# Models to tune\n",
    "MODEL_TYPES = ['NHITS', 'TIMESNET']\n",
    "\n",
    "# Number of trials per model (passed to hpo_tuner.py)\n",
    "N_TRIALS = 50\n",
    "\n",
    "# SLURM settings\n",
    "PARTITION = 'rtx3080'\n",
    "TIME_LIMIT = '10:00:00'\n",
    "GRES = 'gpu:1'\n",
    "CPUS = '8'\n",
    "CONDA_ENV = 'myenv'\n",
    "\n",
    "# Paths\n",
    "SLURM_SCRIPT = 'hpo_job.slurm'\n",
    "JOB_MAP_FILE = 'hpo_current_jobs.json'\n",
    "\n",
    "print('Config set:', MODEL_TYPES, N_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ae237",
   "metadata": {},
   "source": [
    "## 3. Submit SLURM HPO Jobs (one per model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a8f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting HPO for NHITS ...\n",
      "  ✓ Submitted NHITS as Job 1471742\n",
      "    Logs: hpo_1471742.log / hpo_1471742.err\n",
      "Submitting HPO for TIMESNET ...\n",
      "  ✓ Submitted TIMESNET as Job 1471743\n",
      "    Logs: hpo_1471743.log / hpo_1471743.err\n",
      "Saved job map to hpo_current_jobs.json\n"
     ]
    }
   ],
   "source": [
    "job_map = {}\n",
    "for model in MODEL_TYPES:\n",
    "    print(f'Submitting HPO for {model} ...')\n",
    "    result = subprocess.run(\n",
    "        ['sbatch',\n",
    "         '--partition', PARTITION,\n",
    "         '--time', TIME_LIMIT,\n",
    "         '--gres', GRES,\n",
    "         '--cpus-per-task', CPUS,\n",
    "         SLURM_SCRIPT,\n",
    "         model, str(N_TRIALS)],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        job_id = result.stdout.strip().split()[-1]\n",
    "        job_map[model] = job_id\n",
    "        print(f'  ✓ Submitted {model} as Job {job_id}')\n",
    "        print(f'    Logs: hpo_{job_id}.log / hpo_{job_id}.err')\n",
    "    else:\n",
    "        print(f'  ✗ Submission failed for {model}: {result.stderr}')\n",
    "\n",
    "if job_map:\n",
    "    with open(JOB_MAP_FILE, 'w') as f:\n",
    "        json.dump(job_map, f, indent=2)\n",
    "    print('Saved job map to', JOB_MAP_FILE)\n",
    "else:\n",
    "    print('No jobs submitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9b1f9",
   "metadata": {},
   "source": [
    "## 4. Monitor Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86fbc176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking NHITS (Job 1471742) ...\n",
      "  ⏹️  Not in queue (completed or failed)\n",
      "  Tail log:\n",
      "= Node list          : tg081\n",
      "= Subm/Elig/Start/End: 2025-12-21T18:13:12 / 2025-12-21T18:13:12 / 2025-12-21T18:13:13 / 2025-12-21T20:41:17\n",
      "======================\n",
      "=== Quota infos ======\n",
      "    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    \n",
      "    /home/woody           171.0M  1000.0G  1500.0G        N/A   9,028    5,000K   7,500K        N/A    \n",
      "    /home/hpc              40.1G   104.9G   209.7G        N/A     150K     500K   1,000K        N/A    \n",
      "    /home/vault             0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    \n",
      "======================\n",
      "=== GPU utilization ==\n",
      "gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]\n",
      "NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 402043, 67 %, 19 %, 1572 MiB, 8789107 ms\n",
      "\n",
      "  Tail err:\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-12-21 20:41:10,042] Trial 49 finished with value: 0.05893823451283238 and parameters: {'num_stacks': 5, 'num_blocks': 1, 'num_layers': 3, 'layer_widths': 256, 'lr': 0.009094577414994805, 'dropout': 0.15190536389731996, 'weight_decay': 5.195680814779473e-06}. Best is trial 44 with value: 0.03213929689287901.\n",
      "\n",
      "Checking TIMESNET (Job 1471743) ...\n",
      "  ⏹️  Not in queue (completed or failed)\n",
      "  Tail log:\n",
      "= Total RAM usage    : 3.6 GiB of requested  GiB (%)   \n",
      "= Node list          : tg083\n",
      "= Subm/Elig/Start/End: 2025-12-21T18:13:12 / 2025-12-21T18:13:12 / 2025-12-21T18:13:13 / 2025-12-21T20:20:08\n",
      "======================\n",
      "=== Quota infos ======\n",
      "    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    \n",
      "    /home/hpc              40.2G   104.9G   209.7G        N/A     150K     500K   1,000K        N/A    \n",
      "    /home/woody           171.0M  1000.0G  1500.0G        N/A   9,028    5,000K   7,500K        N/A    \n",
      "======================\n",
      "=== GPU utilization ==\n",
      "gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]\n",
      "NVIDIA GeForce RTX 3080, 00000000:3D:00.0, 298079, 92 %, 30 %, 4784 MiB, 7519929 ms\n",
      "\n",
      "  Tail err:\n",
      "`Trainer.fit` stopped: `max_steps=500` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-12-21 20:19:58,669] Trial 49 finished with value: 0.15841809915986058 and parameters: {'hidden_size': 128, 'conv_hidden_size': 64, 'top_k': 3, 'lr': 2.9445601459148115e-05, 'dropout': 0.10744643152017859}. Best is trial 25 with value: 0.12671002416205765.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(JOB_MAP_FILE):\n",
    "    with open(JOB_MAP_FILE, 'r') as f:\n",
    "        job_map = json.load(f)\n",
    "else:\n",
    "    job_map = {}\n",
    "\n",
    "if not job_map:\n",
    "    print('No job map found. Submit jobs first (Section 3).')\n",
    "else:\n",
    "    for model, job_id in job_map.items():\n",
    "        print(f'Checking {model} (Job {job_id}) ...')\n",
    "        res = subprocess.run(\n",
    "            ['squeue', '-j', job_id, '--format=%.18i %.9P %.20j %.8u %.8T %.10M %.9l %.6D %R'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        lines = res.stdout.strip().split('\\n')\n",
    "        if len(lines) <= 1:\n",
    "            print('  ⏹️  Not in queue (completed or failed)')\n",
    "        else:\n",
    "            print(res.stdout)\n",
    "        log_file = f'hpo_{job_id}.log'\n",
    "        if os.path.exists(log_file):\n",
    "            print('  Tail log:')\n",
    "            tail = subprocess.run(['tail', '-n', '12', log_file], capture_output=True, text=True)\n",
    "            print(tail.stdout)\n",
    "        err_file = f'hpo_{job_id}.err'\n",
    "        if os.path.exists(err_file):\n",
    "            err_tail = subprocess.run(['tail', '-n', '6', err_file], capture_output=True, text=True)\n",
    "            if err_tail.stdout.strip():\n",
    "                print('  Tail err:')\n",
    "                print(err_tail.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef4356",
   "metadata": {},
   "source": [
    "## 5. Inspect Saved Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb08c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHITS best params (from results/best_params_NHITS.json):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_stacks</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_blocks</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_layers</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_widths</th>\n",
       "      <td>256.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout</th>\n",
       "      <td>0.110749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decay</th>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0\n",
       "num_stacks      4.000000\n",
       "num_blocks      1.000000\n",
       "num_layers      3.000000\n",
       "layer_widths  256.000000\n",
       "lr              0.000064\n",
       "dropout         0.110749\n",
       "weight_decay    0.000003"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESNET best params (from results/best_params_TIMESNET.json):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hidden_size</th>\n",
       "      <td>256.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv_hidden_size</th>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_k</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout</th>\n",
       "      <td>0.123281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "hidden_size       256.000000\n",
       "conv_hidden_size   64.000000\n",
       "top_k               3.000000\n",
       "lr                  0.000017\n",
       "dropout             0.123281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_files = {\n",
    "    'NHITS': 'results/best_params_NHITS.json',\n",
    "    'TIMESNET': 'results/best_params_TIMESNET.json'\n",
    "}\n",
    "\n",
    "for model, path in best_files.items():\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            params = json.load(f)\n",
    "        print(f'{model} best params (from {path}):')\n",
    "        display(pd.DataFrame(params, index=[0]).T)\n",
    "    else:\n",
    "        print(f'{model}: best params file not found at {path}')\n",
    "        print('Submit/monitor jobs first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57737bf8",
   "metadata": {},
   "source": [
    "## 6. Optional: Run Local HPO (small quick test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small local run (may be slow).\n",
    "# Adjust model and trials as needed.\n",
    "LOCAL_MODEL = 'NHITS'\n",
    "LOCAL_TRIALS = 5\n",
    "\n",
    "print(f'Running local HPO for {LOCAL_MODEL} with {LOCAL_TRIALS} trials...')\n",
    "res = subprocess.run(\n",
    "    ['python3', 'hpo_tuner.py', LOCAL_MODEL, str(LOCAL_TRIALS)],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(res.stdout)\n",
    "if res.returncode != 0:\n",
    "    print('Error:')\n",
    "    print(res.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb84c5",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "- After HPO completes, rerun the main benchmark notebook; it will automatically pick up best params from `results/best_params_*.json`.\n",
    "- Keep logs for reference: `hpo_<JOBID>.log` and `hpo_<JOBID>.err`.\n",
    "- Tune `N_TRIALS`, `TIME_LIMIT`, and SLURM resources above as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
