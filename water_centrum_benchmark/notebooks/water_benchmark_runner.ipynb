{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908be91e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775ca237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n",
      "Current working directory: /home/hpc/iwi5/iwi5389h/ExAI-Timeseries-Thesis/water_centrum_benchmark/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b719b5",
   "metadata": {},
   "source": [
    "## 2. Configure Benchmark Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a149423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data file found: ../../processing/centrum_processing/centrum_features_engineered_from_2018-04-01.csv\n",
      "  Columns: ['timestamp', 'water_consumption', 'temp', 'dew_point', 'humidity']... (total: 27)\n",
      "  Target: water_consumption\n",
      "‚úì Results directory ready: ../results\n",
      "‚úì Benchmarking models: NHITS_Q, TIMESNET_Q\n",
      "\n",
      "üí° Strategy: Split submission - run NHITS, then change line and run TimesNet\n"
     ]
    }
   ],
   "source": [
    "# Data and results paths (relative to project root)\n",
    "DATA_PATH = \"../../processing/centrum_processing/centrum_features_engineered_from_2018-04-01.csv\"\n",
    "RESULTS_DIR = \"../results\"\n",
    "SCRIPTS_DIR = \"../scripts\"\n",
    "\n",
    "# ========================================\n",
    "# MODELS TO BENCHMARK - Choose strategy:\n",
    "# ========================================\n",
    "# Option 1: All models in one job (may take 15-20 hours)\n",
    "# MODELS_TO_RUN = [\"NHITS_Q\", \"NHITS_MSE\", \"TIMESNET_Q\", \"TIMESNET_MSE\"]\n",
    "\n",
    "# Option 2: Split by architecture (RECOMMENDED - run both jobs in parallel)\n",
    "# Job 1: NHITS models (~5-10 hours)\n",
    "MODELS_TO_RUN = [\"NHITS_Q\", \"TIMESNET_Q\"]\n",
    "\n",
    "# Job 2: TimesNet models (~5-10 hours) - Change to this and run cell 3 again\n",
    "# MODELS_TO_RUN = [\"TIMESNET_Q\", \"TIMESNET_MSE\"]\n",
    "\n",
    "# SLURM job configuration\n",
    "SLURM_SCRIPT = \"../scripts/benchmark_water_job.slurm\"\n",
    "JOB_ID_FILE = \"../scripts/current_job_id.txt\"\n",
    "\n",
    "# Verify data file exists\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(f\"‚úì Data file found: {DATA_PATH}\")\n",
    "    df_info = pd.read_csv(DATA_PATH, nrows=5)\n",
    "    print(f\"  Columns: {list(df_info.columns[:5])}... (total: {len(df_info.columns)})\")\n",
    "    print(f\"  Target: water_consumption\")\n",
    "else:\n",
    "    print(f\"‚úó Data file not found: {DATA_PATH}\")\n",
    "    \n",
    "# Create results directory if needed\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"‚úì Results directory ready: {RESULTS_DIR}\")\n",
    "print(f\"‚úì Benchmarking models: {', '.join(MODELS_TO_RUN)}\")\n",
    "print(f\"\\nüí° Strategy: Split submission - run NHITS, then change line and run TimesNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1aebd",
   "metadata": {},
   "source": [
    "## 3. Submit SLURM Benchmark Job\n",
    "\n",
    "**RECOMMENDED**: Submit to SLURM for full benchmarking on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfba3915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting water benchmarker job to SLURM...\n",
      "Models to run: NHITS_Q, TIMESNET_Q\n",
      "Script: ../scripts/benchmark_water_job.slurm\n",
      "\n",
      "‚úì Job submitted successfully!\n",
      "  Job ID: 1478408\n",
      "  Models: NHITS_Q, TIMESNET_Q\n",
      "  Log files: ../scripts/benchmark_1478408.log / benchmark_1478408.err\n",
      "\n",
      "üí° Next: Run cell below to monitor job status\n"
     ]
    }
   ],
   "source": [
    "# Submit benchmarker to SLURM\n",
    "print(\"Submitting water benchmarker job to SLURM...\")\n",
    "print(f\"Models to run: {', '.join(MODELS_TO_RUN)}\")\n",
    "print(f\"Script: {SLURM_SCRIPT}\")\n",
    "\n",
    "# Convert models list to space-separated string for shell argument\n",
    "models_arg = \" \".join(MODELS_TO_RUN)\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"sbatch\", SLURM_SCRIPT, models_arg],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    cwd=\"../scripts\"\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    # Extract job ID from output: \"Submitted batch job 123456\"\n",
    "    job_id = result.stdout.strip().split()[-1]\n",
    "    print(f\"\\n‚úì Job submitted successfully!\")\n",
    "    print(f\"  Job ID: {job_id}\")\n",
    "    print(f\"  Models: {', '.join(MODELS_TO_RUN)}\")\n",
    "    print(f\"  Log files: {SCRIPTS_DIR}/benchmark_{job_id}.log / benchmark_{job_id}.err\")\n",
    "    print(f\"\\nüí° Next: Run cell below to monitor job status\")\n",
    "    \n",
    "    # Save job ID for monitoring\n",
    "    with open(JOB_ID_FILE, \"w\") as f:\n",
    "        f.write(job_id)\n",
    "else:\n",
    "    print(f\"‚úó Error submitting job:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d36b28",
   "metadata": {},
   "source": [
    "## 4. Monitor Job Status\n",
    "\n",
    "Check if the benchmarker job is running or complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9861a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking status of Job 1478408...\n",
      "======================================================================\n",
      "             JOBID PARTITION                 NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n",
      "           1478408      a100      benchmark_water iwi5389h  PENDING       0:00  10:00:00      1 (Resources)\n",
      "\n",
      "\n",
      "‚è≥ Job is still running. Re-run this cell to check again.\n",
      "   Estimated time: ~2-3 hours for all 4 models\n"
     ]
    }
   ],
   "source": [
    "# Check SLURM job status\n",
    "if os.path.exists(JOB_ID_FILE):\n",
    "    with open(JOB_ID_FILE, \"r\") as f:\n",
    "        job_id = f.read().strip()\n",
    "else:\n",
    "    print(\"‚ùå No job ID found. Run cell above to submit the job.\")\n",
    "    job_id = None\n",
    "\n",
    "if job_id:\n",
    "    print(f\"Checking status of Job {job_id}...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check with squeue\n",
    "    result = subprocess.run(\n",
    "        [\"squeue\", \"-j\", job_id, \"--format=%.18i %.9P %.20j %.8u %.8T %.10M %.9l %.6D %R\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if \"Invalid job id\" in result.stderr or not result.stdout.strip().split('\\n')[1:]:\n",
    "        print(f\"‚èπÔ∏è  Job {job_id} is no longer in queue (completed or failed)\")\n",
    "        \n",
    "        # Check if results exist\n",
    "        results_file = os.path.join(RESULTS_DIR, \"benchmark_results.csv\")\n",
    "        if os.path.exists(results_file):\n",
    "            print(f\"\\n‚úÖ Job completed successfully!\")\n",
    "            print(f\"   Results file found: {results_file}\")\n",
    "            print(f\"\\nüí° Next: Continue to next cell to view results\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Job completed but results not found!\")\n",
    "            print(f\"   Check log files: {SCRIPTS_DIR}/benchmark_{job_id}.log\")\n",
    "    else:\n",
    "        print(result.stdout)\n",
    "        print(f\"\\n‚è≥ Job is still running. Re-run this cell to check again.\")\n",
    "        print(f\"   Estimated time: ~2-3 hours for all 4 models\")\n",
    "        \n",
    "    # Show recent log output\n",
    "    log_file = f\"{SCRIPTS_DIR}/benchmark_{job_id}.log\"\n",
    "    if os.path.exists(log_file):\n",
    "        print(f\"\\nüìÑ Recent log output (last 15 lines):\")\n",
    "        print(\"-\"*70)\n",
    "        result = subprocess.run([\"tail\", \"-n\", \"15\", log_file], capture_output=True, text=True)\n",
    "        print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4c304",
   "metadata": {},
   "source": [
    "## 5. Display Benchmark Results\n",
    "\n",
    "Load and display the comprehensive metrics comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b21537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERROR: No benchmark results found!\n",
      "   Searched for: ../results/benchmark_results_*_Water_Centrum_*.csv\n",
      "\n",
      "üí° Solution:\n",
      "   - Check cell above to see if job is still running\n",
      "   - If job completed, check log files for errors\n"
     ]
    }
   ],
   "source": [
    "# Load and display latest results\n",
    "import glob\n",
    "\n",
    "# Find the latest timestamped results file\n",
    "results_pattern = os.path.join(RESULTS_DIR, \"benchmark_results_*_Water_Centrum_*.csv\")\n",
    "result_files = sorted(glob.glob(results_pattern))\n",
    "\n",
    "if not result_files:\n",
    "    print(\"‚ùå ERROR: No benchmark results found!\")\n",
    "    print(f\"   Searched for: {results_pattern}\")\n",
    "    print(\"\\nüí° Solution:\")\n",
    "    print(\"   - Check cell above to see if job is still running\")\n",
    "    print(\"   - If job completed, check log files for errors\")\n",
    "else:\n",
    "    # Load the most recent results file\n",
    "    latest_results_file = result_files[-1]\n",
    "    results_df = pd.read_csv(latest_results_file)\n",
    "    \n",
    "    print(f\"\\nüìä WATER CONSUMPTION BENCHMARK RESULTS\")\n",
    "    print(f\"Loaded from: {os.path.basename(latest_results_file)}\")\n",
    "    print(\"=\"*70)\n",
    "    display(results_df)\n",
    "    \n",
    "    # Highlight best performer for each metric\n",
    "    print(\"\\nüèÜ Best Performer by Metric:\")\n",
    "    for metric in ['MAE', 'RMSE', 'MAPE', 'MIW', 'CRPS']:\n",
    "        if metric in results_df.columns:\n",
    "            best_model = results_df.loc[results_df[metric].idxmin(), 'Model']\n",
    "            best_value = results_df[metric].min()\n",
    "            print(f\"  {metric:8s}: {best_model:12s} ({best_value:.4f})\")\n",
    "    \n",
    "    # For PICP, closer to 80% is better\n",
    "    if 'PICP' in results_df.columns:\n",
    "        target_picp = 80.0\n",
    "        results_df['PICP_diff'] = (results_df['PICP'] - target_picp).abs()\n",
    "        best_model = results_df.loc[results_df['PICP_diff'].idxmin(), 'Model']\n",
    "        best_value = results_df.loc[results_df['PICP_diff'].idxmin(), 'PICP']\n",
    "        print(f\"  PICP    : {best_model:12s} ({best_value:.2f}% - closest to 80%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97dbc41",
   "metadata": {},
   "source": [
    "## 6. Generate Visualizations\n",
    "\n",
    "Create comprehensive visualizations comparing all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7104fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations using dataset-specific results folder\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "# Call visualize_benchmark.py with the dataset-specific results folder\n",
    "result = subprocess.run(\n",
    "    [\"python3\", \"../../visualize_benchmark.py\", os.path.abspath(RESULTS_DIR)],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úì Visualizations generated successfully!\")\n",
    "else:\n",
    "    print(f\"‚úó Error generating visualizations:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c352560",
   "metadata": {},
   "source": [
    "### 6.1 Metrics Comparison Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_plot = os.path.join(RESULTS_DIR, \"benchmark_metrics_barplots.png\")\n",
    "if os.path.exists(metrics_plot):\n",
    "    display(Image(filename=metrics_plot))\n",
    "else:\n",
    "    print(f\"Plot not found: {metrics_plot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bea16",
   "metadata": {},
   "source": [
    "### 6.2 Box Plots - Prediction Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_img = os.path.join(RESULTS_DIR, \"benchmark_boxplots.png\")\n",
    "if os.path.exists(boxplot_img):\n",
    "    display(Image(filename=boxplot_img))\n",
    "else:\n",
    "    print(f\"Plot not found: {boxplot_img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5cc825",
   "metadata": {},
   "source": [
    "### 6.3 Side-by-Side Time Series Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65431172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sidebyside_img = os.path.join(RESULTS_DIR, \"benchmark_comparison_sidebyside.png\")\n",
    "if os.path.exists(sidebyside_img):\n",
    "    display(Image(filename=sidebyside_img))\n",
    "else:\n",
    "    print(f\"Plot not found: {sidebyside_img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102ce19",
   "metadata": {},
   "source": [
    "## 7. View Benchmark History\n",
    "\n",
    "Track improvements over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View benchmark history across all runs for WATER dataset\n",
    "history_file = \"../../results/benchmark_history.csv\"\n",
    "\n",
    "if os.path.exists(history_file):\n",
    "    history = pd.read_csv(history_file)\n",
    "    \n",
    "    # Filter for water dataset only\n",
    "    water_history = history[history['dataset'] == 'Water (Centrum)']\n",
    "    \n",
    "    if len(water_history) == 0:\n",
    "        print(\"‚ùå No water benchmark history found yet.\")\n",
    "        print(\"   Run a benchmark first to start tracking results.\")\n",
    "    else:\n",
    "        print(\"üìä WATER BENCHMARK HISTORY - ALL RUNS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total runs recorded: {len(water_history)}\")\n",
    "        print(f\"Models tracked: {', '.join(water_history['Model'].unique())}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # Show all runs with key metrics\n",
    "        display_cols = ['run_date', 'Model', 'n_epochs', 'has_hpo', 'MAE', 'RMSE', 'MAPE', 'CRPS']\n",
    "        available_cols = [col for col in display_cols if col in water_history.columns]\n",
    "        display(water_history[available_cols].sort_values('run_date', ascending=False))\n",
    "        \n",
    "        # Show best results ever\n",
    "        print(\"\\nüèÜ BEST RESULTS EVER ACHIEVED\")\n",
    "        print(\"=\"*70)\n",
    "        for model in water_history['Model'].unique():\n",
    "            model_history = water_history[water_history['Model'] == model]\n",
    "            print(f\"\\n{model}:\")\n",
    "            for metric in ['MAE', 'RMSE', 'MAPE', 'CRPS']:\n",
    "                if metric in model_history.columns:\n",
    "                    best_idx = model_history[metric].idxmin()\n",
    "                    best_val = model_history.loc[best_idx, metric]\n",
    "                    best_date = model_history.loc[best_idx, 'run_date']\n",
    "                    print(f\"  {metric:8s}: {best_val:.4f} (achieved on {best_date})\")\n",
    "else:\n",
    "    print(\"‚ùå No benchmark history found.\")\n",
    "    print(f\"   Expected file: {history_file}\")\n",
    "    print(\"\\nüí° Run a benchmark first to start tracking results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9ba69",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "Compare these results with the heat benchmarking results to analyze domain differences.\n",
    "\n",
    "**Next Steps:**\n",
    "- Review individual model plots for detailed error analysis\n",
    "- Compare with heat benchmark results (in `nordbyen_heat_benchmark/results/`)\n",
    "- Consider hyperparameter optimization if results need improvement\n",
    "- Examine specific time periods where models differ significantly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
